{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Exee-KpDaeBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtGZOmFAYqpl"
      },
      "outputs": [],
      "source": [
        "def integration_task(seq_len, num_sapmles):\n",
        "  for i in range(num_samples):\n",
        "    noise = (np.random.rand(seq_len) * 2 - 1).reshape((seq_len, 1))\n",
        "    target = int(np.sum(noise) >= 0)\n",
        "    yield noise, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 25\n",
        "num_samples = 80000\n",
        "\n",
        "def my_integration_task():\n",
        "  for i in integration_task(seq_len, num_samples):\n",
        "    yield i"
      ],
      "metadata": {
        "id": "-VSyT-zFdWGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_generator(my_integration_task, output_signature=(\n",
        "         tf.TensorSpec(shape=(seq_len,1), dtype=tf.float64),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "def prepare_integration_data(integration_ds):\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  integration_ds = integration_ds.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  integration_ds = integration_ds.shuffle(100)\n",
        "  integration_ds = integration_ds.batch(32)\n",
        "  integration_ds = integration_ds.prefetch(100)\n",
        "  #return preprocessed dataset\n",
        "  return integration_ds\n",
        "\n",
        "ds = prepare_integration_data(ds)\n",
        "train_ds = ds.take(70000)\n",
        "test_ds = ds.take(10000)"
      ],
      "metadata": {
        "id": "R8JuQEDBe02L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Cell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, kernel_regularizer=None ):\n",
        "        super(LSTM_Cell, self).__init__()\n",
        "        \n",
        "        self.units = units\n",
        "        \n",
        "        self.dense_forget = tf.keras.layers.Dense(units, \n",
        "                                                 kernel_regularizer=kernel_regularizer, \n",
        "                                                 use_bias=False)\n",
        "        self.forget_bias = tf.Variable(tf.ones(units), name=\"LSTM_forget_biases\")\n",
        "        \n",
        "        self.dense_input = tf.keras.layers.Dense(units, \n",
        "                                                 kernel_regularizer=kernel_regularizer, \n",
        "                                                 use_bias=False)\n",
        "        self.input_bias = tf.Variable(tf.zeros(units), name=\"LSTM_input_biases\")        \n",
        "        \n",
        "        self.dense_cell = tf.keras.layers.Dense(units, \n",
        "                                                 kernel_regularizer=kernel_regularizer, \n",
        "                                                 use_bias=False)\n",
        "        self.cell_bias = tf.Variable(tf.zeros(units), name=\"LSTM_cell_biases\")\n",
        "        \n",
        "        self.dense_output = tf.keras.layers.Dense(units, \n",
        "                                                 kernel_regularizer=kernel_regularizer, \n",
        "                                                 use_bias=False)\n",
        "        self.output_bias = tf.Variable(tf.zeros(units), name=\"LSTM_output_biases\")\n",
        "        \n",
        "        self.dense_hidden = tf.keras.layers.Dense(units, \n",
        "                                                 kernel_regularizer=kernel_regularizer, \n",
        "                                                 use_bias=False)\n",
        "        self.hidden_bias = tf.Variable(tf.zeros(units), name=\"LSTM_hidden_biases\")\n",
        "        \n",
        "        self.state_size = units\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, input_t, state, training=False):\n",
        "        \n",
        "        f_t = tf.keras.activations.sigmoid(self.dense_forget(tf.concat(state[0], input_t)) + self.forget_bias)\n",
        "        \n",
        "        i_t = tf.keras.activations.sigmoid(self.dense_input(tf.concat(state[0], input_t)) + self.input_bias)\n",
        "        \n",
        "        C_t = tf.keras.activations.tanh(self.dense_cell(tf.concat(state[0], input_t)) + self.cell_bias)\n",
        "\n",
        "        cell_state = tf.linalg.matmul(f_t, state[1]) + tf.linalg.matmul(i_t, C_t)\n",
        "\n",
        "        o_t = tf.keras.activations.sigmoid(self.dense_output(tf.concat(state[0], input_t)) + self.output_bias)\n",
        "\n",
        "        hidden_state = tf.linalg.matmul(o_t, tf.keras.activations.tanh(cell_state))\n",
        "\n",
        "        return (hidden_state, cell_state)"
      ],
      "metadata": {
        "id": "bDuL947npfyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNWrapper(tf.keras.layers.Layer):\n",
        "    def __init__(self, RNN_Cell, return_sequences=False):\n",
        "        super(RNNWrapper, self).__init__()\n",
        "        \n",
        "        self.return_sequences = return_sequences\n",
        "        \n",
        "        self.cell = RNN_Cell\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, data, training=False):\n",
        "        \n",
        "        length = data.shape[1]\n",
        "\n",
        "        # initialize state of the simple rnn cell\n",
        "        state = tf.zeros((data.shape[0], self.cell.units), tf.float32)\n",
        "        \n",
        "        # initialize array for hidden states (only relevant if self.return_sequences == True)\n",
        "        hidden_states = tf.TensorArray(dtype=tf.float32, size=length)\n",
        "\n",
        "        for t in tf.range(length):\n",
        "            input_t = data[:,t,:]\n",
        "\n",
        "            state = self.cell(input_t, state, training)\n",
        "\n",
        "            if self.return_sequences:\n",
        "                # write the states to the TensorArray\n",
        "                #hidden_states = hidden_states.write(t, state)\n",
        "                hidden_states.append(state)\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            # transpose the sequence of hidden_states from TensorArray accordingly \n",
        "            #(batch and time dimensions are otherwise switched after .stack())\n",
        "            outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
        "        \n",
        "        else:\n",
        "            # take the last hidden state of the simple rnn cell\n",
        "            outputs = state\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "hafsDqUEZI8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Model(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(RNN_Model, self).__init__()\n",
        "\n",
        "        self.RNNWrapper = RNNWrapper(CustomSimpleRNNCell(units), return_sequences=False)\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, data, training=False):\n",
        "        \n",
        "        x = self.RNNWrapper(data, training)\n",
        "        x = tf.keras.activations.sigmoid(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "h_GZ3o-3ZCiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction[:,(seq_len-1)])\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function, accuracy_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction[:,(seq_len-1)])\n",
        "    sample_test_accuracy = accuracy_function.update_state(target, prediction[:,(seq_len-1)])\n",
        "    sample_test_accuracy = accuracy_function.result()\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(sample_test_accuracy.numpy())\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "DtcDNWHlreeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "#train_dataset = train_dataset.take(1000)\n",
        "#test_dataset = test_dataset.take(100)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 15\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Initialize the model.\n",
        "model = RNN_Model(25)\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "accuracy_function = tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=0.5)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_ds, cross_entropy_loss, accuracy_function)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_ds, cross_entropy_loss, accuracy_function)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_ds:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "\n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_ds, cross_entropy_loss, accuracy_function)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "metadata": {
        "id": "SGdRveyfgkcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd90eb1c-5a4b-4282-99bc-2f869ed882b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 starting with accuracy 0.5577328205108643\n",
            "Epoch: 1 starting with accuracy 0.6235326528549194\n",
            "Epoch: 2 starting with accuracy 0.7120217084884644\n",
            "Epoch: 3 starting with accuracy 0.7625622749328613\n",
            "Epoch: 4 starting with accuracy 0.7946837544441223\n",
            "Epoch: 5 starting with accuracy 0.8171601295471191\n",
            "Epoch: 6 starting with accuracy 0.8345286250114441\n",
            "Epoch: 7 starting with accuracy 0.8487794995307922\n",
            "Epoch: 8 starting with accuracy 0.8598180413246155\n",
            "Epoch: 9 starting with accuracy 0.8683062791824341\n",
            "Epoch: 10 starting with accuracy 0.8754562735557556\n",
            "Epoch: 11 starting with accuracy 0.8813861608505249\n",
            "Epoch: 12 starting with accuracy 0.8862486481666565\n",
            "Epoch: 13 starting with accuracy 0.8896233439445496\n",
            "Epoch: 14 starting with accuracy 0.892823338508606\n"
          ]
        }
      ]
    }
  ]
}