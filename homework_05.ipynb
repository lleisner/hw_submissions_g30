{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309be51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84552861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:19:13.372595: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad136dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1644d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1644d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1644d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1640e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1640e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad1640e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function prepare_mnist_fashion_data.<locals>.<lambda> at 0x7f9cad164b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = tfds.load('fashion_mnist', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "def prepare_mnist_fashion_data(mnist):\n",
    "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
    "  mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "  mnist = mnist.cache()\n",
    "  mnist = mnist.shuffle(1000)\n",
    "  mnist = mnist.batch(8)\n",
    "  mnist = mnist.prefetch(20)\n",
    "  return mnist\n",
    "\n",
    "train_dataset = train_ds.apply(prepare_mnist_fashion_data)\n",
    "test_dataset = test_ds.apply(prepare_mnist_fashion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ef7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32, kernel_size=3, strides=(1, 1), padding='valid',\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.maxPool1 = tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(2, 2), strides=(1,1), padding='valid'\n",
    "        )\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(100, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.maxPool1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ad694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = model(input)\n",
    "    loss = loss_function(target, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "def test(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "\n",
    "  test_accuracy_aggregator = []\n",
    "  test_loss_aggregator = []\n",
    "\n",
    "  for (input, target) in test_data:\n",
    "    prediction = model(input)\n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6324fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9cad436bd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9cad436bd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method MyModel.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9cad436bd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:19:24.527157: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:24.528340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:28.222529: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:28.229531: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 starting with accuracy 0.12875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:19:42.706015: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:42.713848: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:43.197467: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:43.198749: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 starting with accuracy 0.75625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:19:58.999215: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:59.007421: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:59.543618: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:19:59.544950: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 starting with accuracy 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:20:15.907130: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:15.915017: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:16.396552: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:16.397840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 starting with accuracy 0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:20:32.317423: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:32.328406: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:32.894308: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:32.895609: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 starting with accuracy 0.82125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:20:48.917309: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:48.924677: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:49.464888: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:20:49.466074: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 starting with accuracy 0.83125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:21:05.409338: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:05.417282: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:05.914550: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:05.915868: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 starting with accuracy 0.83375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:21:21.910733: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:21.918494: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:22.469963: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:22.471362: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 starting with accuracy 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:21:38.473173: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:38.480864: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:38.977115: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:38.978399: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 starting with accuracy 0.84625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:21:54.552493: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:54.560219: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:55.043714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:21:55.045181: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 starting with accuracy 0.84125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 16:22:11.015898: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:22:11.023735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:22:11.515093: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-12-05 16:22:11.516431: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
    "train_dataset = train_dataset.take(1000)\n",
    "test_dataset = test_dataset.take(100)\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "# Initialize the model.\n",
    "model = MyModel()\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate, momentum)\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "#testing once before we begin\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "#check how model performs on train data once before we begin\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "# We train for num_epochs epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
    "\n",
    "    #training (and checking in with training)\n",
    "    epoch_loss_agg = []\n",
    "    for input,target in train_dataset:\n",
    "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
    "        epoch_loss_agg.append(train_loss)\n",
    "    \n",
    "    #track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    #testing, so we can track accuracy and test loss\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5a9f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize accuracy and loss for training and test data.\n",
    "plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(test_losses)\n",
    "line3, = plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f27c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
